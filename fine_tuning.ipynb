{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to load the LLM\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "# Set notebook name for wandb (adjust as needed)\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"QLoRA_v3.0.ipynb\"\n",
    "# Set the WANDB_API_KEY environment variable\n",
    "os.environ[\"WANDB_API_KEY\"] = \"YOUR WANDB API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets\n",
    "ds_train_id = \"/home/czy/Project-1 LLMs Database/Richness/3/ds_train.xlsx\"\n",
    "c\n",
    "# Test datasets\n",
    "ds_test_id = \"/home/czy/Project-1 LLMs Database/Richness/3/ds_test.xlsx\"\n",
    "\n",
    "# Pre-trained model (adjust as needed)\n",
    "model_path = \"/home/czy/Project-1 LLMs Database/meta_llama_2_7b_chat\"\n",
    "\n",
    "# Address for storaging model weight (adjust as needed)\n",
    "adapter_path = \"/home/czy/Project-1 LLMs Database/lora_adapter\"\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = \"\"\"<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "question: {user_query}\n",
    "context: {context}[/INST] {response}</s>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base model and tokenizer of LLaMA2-7B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer of LLaMA2-Chat-7B \n",
    "def load_tokenizer(pretrained_path: str) -> Any:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path = pretrained_path,\n",
    "        padding_side = \"left\",\n",
    "        token = os.environ.get(\"TRANSFORMERS_OFFLINE\"),\n",
    "        local_files_only = True\n",
    "    )\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# Load pretrained model (LLaMA2-7B-Chat)\n",
    "def load_pretrained_model(pretrained_path: str) -> Any:\n",
    "    # Model quantization parameter settings\n",
    "    model_bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant = False\n",
    "    )\n",
    "    \n",
    "    # Load pretrained model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = pretrained_path,\n",
    "        quantization_config = model_bnb_config,\n",
    "        token = os.environ.get(\"TRANSFORMERS_OFFLINE\"),\n",
    "        device_map = \"auto\",\n",
    "        low_cpu_mem_usage = True,\n",
    "        local_files_only = True\n",
    "    )\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_tokenizer = load_tokenizer(pretrained_path=model_path)\n",
    "llama2_tokenizer.pad_token = llama2_tokenizer.eos_token  # Set the fill character to </s>, if set <unk>, model accuracy will reduce\n",
    "llama2_tokenizer.add_eos_token = False  # Don't add </s> at the end of input\n",
    "llama2_tokenizer.add_bos_token = False  # Don't add <s> at the start of input\n",
    "\n",
    "llama2_pretrained = load_pretrained_model(pretrained_path=model_path)\n",
    "llama2_pretrained.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "llama2 = prepare_model_for_kbit_training(llama2_pretrained)  # Note this method only works for transformers models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the inputs before inference\n",
    "def generate_and_tokenize_prompt(data_point: Dict, tokenizer: Any = llama2_tokenizer, template: str = prompt_template) -> Any:\n",
    "    system_prompt = data_point[\"instruction\"]  # or ï»¿instruction\n",
    "    user_query = data_point[\"user_query\"]\n",
    "    reference_context = data_point[\"context\"]\n",
    "    model_reponse = data_point[\"output\"]\n",
    "\n",
    "    # Filling prompt template with data point\n",
    "    prompt = template.format(\n",
    "        system_prompt=system_prompt, \n",
    "        user_query=user_query, \n",
    "        context=reference_context, \n",
    "        response=model_reponse\n",
    "    )\n",
    "    \n",
    "    tokenized_input = tokenizer(\n",
    "        prompt,\n",
    "        truncation = True,  # If the input length exceeds the maximum length, cut off at the boundary\n",
    "        max_length = 2048,\n",
    "        padding = \"max_length\"\n",
    "    )\n",
    "    \n",
    "    tokenized_input[\"labels\"] = tokenized_input[\"input_ids\"].copy()\n",
    "    # tokenized_input[\"labels\"] = torch.tensor(tokenized_input[\"input_ids\"].copy(), dtype=torch.long)\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(ds_train_id)\n",
    "df_test = pd.read_excel(ds_test_id)\n",
    "# Construct train datasets\n",
    "df_train = df_train.sample(n=112, replace=False, random_state=66)  # n here represents the scale of training set\n",
    "df_train.to_excel(\"../Richness/3/tr_112.xlsx\")\n",
    "ds_train = Dataset.from_pandas(df_train)\n",
    "tokenized_ds_train = ds_train.map(generate_and_tokenize_prompt)\n",
    "print(tokenized_ds_train)\n",
    "# Construct test datasets\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "tokenized_ds_test = ds_test.map(generate_and_tokenize_prompt)\n",
    "print(tokenized_ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_ds_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (If there is a demand) Check ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # display all rows\n",
    "pd.set_option('display.max_columns', None)  # display all columns\n",
    "pd.set_option('display.max_colwidth', None)  # display the full content of the cell\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Low-Rank Adapter (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Sharded Data Parallel\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config = FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config = FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
    "\n",
    "# Set LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r = 8,  # Increasing r does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufﬁcient\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],  # it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank\n",
    "    lora_alpha = 8,  # A scaling factor that is used to scale delta W when training\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model_with_peft = get_peft_model(model=llama2, peft_config=peft_config)\n",
    "model_with_peft = accelerator.prepare_model(model=model_with_peft)\n",
    "model_with_peft.print_trainable_parameters()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_with_peft.is_parallelizable = True\n",
    "    model_with_peft.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))\n",
    "run = wandb.init(\n",
    "    project = \"Fine tuning llama-2-7B-chat\",\n",
    "    name = \"r3ds112gn03wr03bs4ep8accelerator\",  # adjust as needed\n",
    "    job_type = \"training\",\n",
    "    anonymous = \"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"journal-finetune\"\n",
    "base_model_name = \"llama2-7b-chat\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    optim = \"paged_adamw_8bit\",             # defaults to \"adamw_torch\"\n",
    "    weight_decay = 0.001,                   # prevent our model from getting too complex, wd * sum(square(all weights)), defaults to 0\n",
    "    max_grad_norm = 0.3,                    # defaults 1.0\n",
    "    warmup_ratio = 0.3,                     # defaults 0.0\n",
    "    lr_scheduler_type = \"linear\",           # defaults \"linear\", optional \"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"\n",
    "    learning_rate = 2e-4,                   # defaults 5e-5 (custom 2e-4)\n",
    "    num_train_epochs = 8,\n",
    "    per_device_train_batch_size = 4,        # defaults 8\n",
    "    gradient_accumulation_steps = 1,        # defaults 1\n",
    "    eval_strategy = \"steps\",\n",
    "    per_device_eval_batch_size = 4,         # defaults 8\n",
    "    eval_accumulation_steps = 1,            # defaults 1\n",
    "    eval_steps = 1,\n",
    "    fp16 = False,\n",
    "    bf16 = False,\n",
    "    seed = 66,\n",
    "    report_to = \"wandb\",\n",
    "    run_name = f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n",
    "    output_dir = output_dir,\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_steps = 1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model_with_peft,\n",
    "    train_dataset = tokenized_ds_train,  \n",
    "    eval_dataset = tokenized_ds_test,\n",
    "    args = training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LLaMA2-7B-Chat by QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PolyLlama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
