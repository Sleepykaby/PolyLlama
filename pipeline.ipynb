{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import PyPDF2\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the random\n",
    "seed = 66\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)  \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Setting huggingface hub access token for LLM\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Initial\n",
    "zero_shot = \"\"\"You are a polymer materials scientist. Your task is to extract specific information from the supplied context based on the question. The extracted information must be returned as a JSON object, including units and values. The outputs should only include the JSON object without any explanation or note. If the supplied context contains the asked values and units, the value corresponding to the JSON key must be replaced with the values and units that appear in the supplied text. If there is no value or unit, fill in the corresponding blank with null. The extracted information must be in the following format: {\"melt temperature\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"mold temperature\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"injection speed\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"injection pressure\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"holding pressure\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"holding time\": {\"unit\": extracted unit, \"value\": [extracted value]}}\"\"\"\n",
    "# 1273 tokens\n",
    "few_shots = \"\"\"You are a polymer materials scientist. Your task is to extract specific information from the supplied context based on the question. The extracted information must be returned as a JSON object, including units and values. The outputs should only include the JSON object without any explanation or note. If the supplied context contains the asked values and units, the value corresponding to the JSON key must be replaced with the values and units that appear in the supplied text. If there is no value or unit, fill in the corresponding blank with null. The extracted information must be in the following format: {\"melt temperature\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"mold temperature\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"injection speed\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"injection pressure\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"holding pressure\": {\"unit\": extracted unit, \"value\": [extracted value]}, \"holding time\": {\"unit\": extracted unit, \"value\": [extracted value]}}. Here are three examples:\n",
    "User: question: In the following context, what are the values and units of melt temperature, mold temperature, injection rate, injection pressure, holding pressure, and holding time? Return as a JSON object. context: The Pellets was dried at 100\\u2103 for 10 h by NER-S10. During injection molding, temperature of melt was set 230 \\u2103, and mold temperature was from 40 to 60 \\u2103. The holding temperature was 100 MPa and duration for 3-5 s.\n",
    "Assistant: {\"melt temperature\": {\"unit\": \"\\u2103\", \"value\": [230]}, \"mold temperature\": {\"unit\": \"\\u2103\", \"value\": [40, 60]}, \"injection rate\": {\"unit\": null, \"value\": null}, \"injection pressure\": {\"unit\": null, \"value\": null}, \"holding pressure\": {\"unit\": \"MPa\", \"value\": [100]}, \"holding time\": {\"unit\": \"s\", \"value\": [3, 5]}}\n",
    "User: question: In the following context, what are the values and units of melt temperature, mold temperature, injection rate, injection pressure, holding pressure, and holding time? Return as a JSON object. context: The holding pressure was controlled at 40 MPa for 5s based on the preliminary results. A machine-set injection pressure of 120 MPa was used for molding, and the injection rate was from 15 to 35 mm/s. Cooling time was 20, 22.5, and 25s. There were four key operating parameters that can affect the formation of polymeric parts.\n",
    "Assistant: {\"melt temperature\": {\"unit\": null, \"value\": null}, \"mold temperature\": {\"unit\": null, \"value\": null}, \"injection rate\": {\"unit\": \"mm/s\", \"value\": [15, 35]}, \"injection pressure\": {\"unit\": null, \"value\": null}, \"holding pressure\": {\"unit\": \"MPa\", \"value\": [40]}, \"holding time\": {\"unit\": \"s\", \"value\": [5]}}\n",
    "User: question: In the following context, what are the values and units of melt temperature, mold temperature, injection rate, injection pressure, holding pressure, and holding time? Return as a JSON object. context: The materials is characterized by a melt flow index of 3g/10min (2.16kg, 230\\u2103, ISO 1133), a weight-average molecular weight approx. 320000 (GPC) and an isotacticity index of 98% (ISO 9113). The morphology of both the \\u03B1 and \\u03B2-iPP specimens is insensitive to holding pressure changes from 50 to 70 MPa.\n",
    "Assistant: {\"melt temperature\": {\"unit\": null, \"value\": null}, \"mold temperature\": {\"unit\": null, \"value\": null}, \"injection rate\": {\"unit\": null, \"value\": null}, \"injection pressure\": {\"unit\": null, \"value\": null}, \"holding pressure\": {\"unit\": \"MPa\", \"value\": [50]}, \"holding time\": {\"unit\": null, \"value\": null}}\"\"\"\n",
    "\n",
    "# query = \"In the following context, what are the values and units of melt temperature, mold temperature, injection speed, injection pressure, holding pressure, and holding time? Return as a JSON object.\"\n",
    "query = \"In the following context, what are the values and units of melt temperature, mold temperature, injection speed, injection pressure, holding pressure, and holding time? Return as a JSON object.\"\n",
    "\n",
    "# Path for storage model outputs\n",
    "output = \"YOUR/OUTPUT/REPO/outputs.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama2(pretrained_model: str, peft_model: str = None):\n",
    "\n",
    "    # Since LLMs are not trained to continue from pad tokens, your input needs to be left-padded.\n",
    "    llama2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path = pretrained_model,\n",
    "        padding_side = \"left\",\n",
    "        local_files_only = True,\n",
    "        token = os.environ.get(\"TRANSFORMERS_OFFLINE\")\n",
    "    )\n",
    "    llama2_tokenizer.pad_token = llama2_tokenizer.eos_token\n",
    "    llama2_tokenizer.add_bos_token = False\n",
    "\n",
    "    # setting quantization params\n",
    "    model_bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant = False\n",
    "    )\n",
    "\n",
    "    # load model from huggingface.co (huggingface_hub)\n",
    "    llama2_model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = pretrained_model,\n",
    "        quantization_config = model_bnb_config,\n",
    "        local_files_only = True,\n",
    "        token = os.environ.get(\"TRANSFORMERS_OFFLINE\"),\n",
    "        device_map = \"auto\",\n",
    "        low_cpu_mem_usage = True\n",
    "    )\n",
    "\n",
    "    if peft_model is not None:\n",
    "        llama2_model = PeftModel.from_pretrained(llama2_model, peft_model)\n",
    "\n",
    "    return llama2_tokenizer, llama2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"YOUR/MODEL/REPO/llama-3.1-8B-Instruct-hf\"\n",
    "tokenizer, model = load_llama2(pretrained_model=MODEL_ID)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Skip when inference) Test performance of LLaMA2-7B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"PA6 was processed by NSR-2S and cycle of whole process is total 25s, the pellets were dried at 80\\u2103 for 6h. During injection molding, a barrel temperature from 180 to 210\\u2103, mould temperature was steady at 60\\u2103, but holding pressure was set from 70 to 100 MPa for 5s.\"\n",
    "\n",
    "chat = [\n",
    "        {\"role\": \"system\", \"content\": f\"{zero_shot}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"question: {query}\\ncontext: {context}\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.apply_chat_template(\n",
    "    chat,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_tokens,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.0,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.01)\n",
    "decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "# complete_output = formatted_chat + decoded_output\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branch I: Loading pdf as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed_model(model_path: str) -> Any:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        local_files_only = True,\n",
    "        token=os.environ.get(\"TRANSFORMERS_OFFLINE\")\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_path,\n",
    "        local_files_only = True,\n",
    "        token=os.environ.get(\"TRANSFORMERS_OFFLINE\")\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def pdf_parser(pdf_path):\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "        cleaned_text = re.sub(r'(?i)REFERENCES.*$', '', text, flags=re.DOTALL)\n",
    "        replaced_text = re.sub(r'(8C|/C14C)', \"\\u2103\", cleaned_text)\n",
    "\n",
    "    return replaced_text\n",
    "\n",
    "def check_illegal_chars(context: str) -> str:\n",
    "        \"\"\"Check for illegal characters in the content and replace them\"\"\"\n",
    "        pattern = re.compile(r'[^\\x09\\x0A\\x0D\\x20-\\uD7FF\\uE000-\\uFFFD\\u10000-\\u10FFFF]', re.UNICODE)\n",
    "        illegal_chars_list = [(m.start(), m.group()) for m in pattern.finditer(context)]\n",
    "        \n",
    "        if illegal_chars_list:\n",
    "            list_context = list(context)\n",
    "\n",
    "            for pos, char in illegal_chars_list:\n",
    "                list_context[pos] = f\"[illegal character: {char}]\"\n",
    "\n",
    "            highlighted_context = ''.join(list_context)\n",
    "            clean_context = re.sub(pattern, \"\", context)\n",
    "            print(f\"Highlight illegal characters:\\n{highlighted_context}\")\n",
    "            print(f\"after treatment:\\n{clean_context}\")\n",
    "\n",
    "            return clean_context\n",
    "        else:\n",
    "            return context\n",
    "\n",
    "def text_splitter(extracted_text: str, size: int, overlap: int) -> List[str]:\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", extracted_text)\n",
    "    chunks = list()\n",
    "    sent_idx = 0\n",
    "    count = len(sentences)\n",
    "\n",
    "    while sent_idx < count:\n",
    "        chunk_end = min(sent_idx + size, count)\n",
    "        chunk = \" \".join(sentences[sent_idx:chunk_end])\n",
    "\n",
    "        if re.search(r\"\\d\", chunk):\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "        sent_idx += (size - overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def relevance_rerank(chunks: List[str], query: str, tokenizer: Any, model: Any) -> Any:\n",
    "    query_embedded = __embedding_input([query], tokenizer, model)\n",
    "    chunks_embedded = __embedding_input(chunks, tokenizer, model)\n",
    "\n",
    "    similarity_scores = np.dot(query_embedded, chunks_embedded.T)\n",
    "\n",
    "    db = pd.DataFrame({\n",
    "        \"similarity_score\": similarity_scores[0],\n",
    "        \"context\": chunks,\n",
    "        \"embedding\": chunks_embedded.tolist()\n",
    "    })\n",
    "\n",
    "    db.sort_values(by=\"similarity_score\", ascending=False, inplace=True)\n",
    "    db.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return db\n",
    "\n",
    "\n",
    "def __embedding_input(text: List[str], tokenizer, model) -> Any:\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    embeddings = __mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def __mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tokenizer, embed_model = load_embed_model(model_path=\"YOUR/MODEL/REPO/all_MiniLM_L6_v2\")  # dim 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"YOUR/PDF/REPO/2015-Elsevier.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = pdf_parser(pdf_path=pdf_path)\n",
    "chunks = text_splitter(extracted_text, 7, 1)\n",
    "db = relevance_rerank(chunks, query, embed_tokenizer, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['context'] = db['context'].apply(check_illegal_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branch II: Loading dataset as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_id = \"YOUR/DATASET/REPO/ds_test.xlsx\"\n",
    "db = pd.read_excel(datasets_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process outputs\n",
    "def outputparser(text: str) -> Dict[str, Any]:\n",
    "    result = re.search(r'<<SYS>>(.*?)<</SYS>>(.*?)question:(.*?)context:(.*?)\\[/INST\\](.*)', text, flags=re.DOTALL)\n",
    "\n",
    "    if result:\n",
    "        sys_info = result.group(1).strip()\n",
    "        question = result.group(3).strip()\n",
    "        context = result.group(4).strip()\n",
    "        output = result.group(5).strip()\n",
    "    else:\n",
    "        sys_info = \"\"\n",
    "        question = \"\"\n",
    "        context = \"\"\n",
    "        output = \"\"\n",
    "\n",
    "    return {\"system\": sys_info, \"user\": {\"question\": question, \"context\": context}, \"assistant\": output}\n",
    "\n",
    "\n",
    "# Storage outputs into Excel\n",
    "def storage(excel_path: str, response: Dict[str, Any]) -> None:\n",
    "    # Load the workbook if it exists, otherwise create a new one\n",
    "    if os.path.exists(excel_path):\n",
    "        workbook = openpyxl.load_workbook(excel_path)\n",
    "        worksheet = workbook.active\n",
    "    else:\n",
    "        workbook = Workbook()\n",
    "        worksheet = workbook.active\n",
    "        worksheet.title = \"Responses\"\n",
    "\n",
    "    # If the worksheet is empty, add headers\n",
    "    if worksheet.max_row == 1 and worksheet.max_column == 1 and worksheet[\"A1\"].value is None:\n",
    "        headers = [\"system\", \"question\", \"context\", \"assistant\"]\n",
    "        worksheet.append(headers)\n",
    "\n",
    "    # Append the new row with response data\n",
    "    new_row = [\n",
    "        response[\"system\"], \n",
    "        response[\"user\"][\"question\"], \n",
    "        response[\"user\"][\"context\"], \n",
    "        response[\"assistant\"]\n",
    "    ]\n",
    "    worksheet.append(new_row)\n",
    "\n",
    "    # Save the workbook\n",
    "    workbook.save(excel_path)\n",
    "\n",
    "\n",
    "# metric for evaluation\n",
    "def compute_metrics(prediction: str, reference: str) -> float:\n",
    "    try:\n",
    "        ref_dict = json.loads(reference)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Reference JSONDecodeError:\", e)\n",
    "    \n",
    "    pattern = r\"\\{.*\\}\"\n",
    "    match = re.search(pattern, prediction, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        try:\n",
    "            pre_dict = json.loads(match.group(0))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Prediction JSONDecodeError:\", e)\n",
    "            return 0.0\n",
    "\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        \n",
    "        for key in ref_dict:\n",
    "            if key in pre_dict:\n",
    "                ref_unit = ref_dict[key].get('unit')\n",
    "                ref_value = ref_dict[key].get('value')\n",
    "                try:\n",
    "                    pre_unit = pre_dict[key].get('unit')\n",
    "                    pre_value = pre_dict[key].get('value')\n",
    "                except AttributeError as e:\n",
    "                    print(f\"{key} AttributeError:\", e)\n",
    "                    pre_unit = False\n",
    "                    pre_value = False\n",
    "\n",
    "                if ref_unit == pre_unit and ref_value == pre_value:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    incorrect += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "        \n",
    "        total = correct + incorrect\n",
    "        return correct / total\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=======================Task start!=======================\")\n",
    "count = 0\n",
    "correct = 0\n",
    "evaluation = list()\n",
    "\n",
    "for i, context in enumerate(db[\"context\"].head(10)):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": f\"{zero_shot}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"question: {query}\\ncontext: {context}\"}\n",
    "    ]\n",
    "\n",
    "    formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.01)\n",
    "    decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "    complete_output = formatted_chat + decoded_output\n",
    "    # print(complete_output)\n",
    "    structured_output = outputparser(complete_output)\n",
    "    # Storage response\n",
    "    storage(excel_path=output, response=structured_output)\n",
    "    \n",
    "    # # Evaluation\n",
    "    # score_i = compute_metrics(prediction=structured_output[\"assistant\"], reference=db[\"output\"][i])\n",
    "    # evaluation.append(score_i)\n",
    "\n",
    "    # if score_i == 1:\n",
    "    #     correct += 1\n",
    "    #     print(True)\n",
    "\n",
    "    count += 1\n",
    "    print(f\"------------------task {count} complete!------------------\")\n",
    "\n",
    "# accuracy = correct/count\n",
    "# print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PolyLlama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
